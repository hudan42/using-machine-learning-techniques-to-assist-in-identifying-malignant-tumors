confusion_tree
nrow(train)
sqrt(455)
# Rescale
# Conduct a KNN classification of the diagnosis variable, set the k parameter to 21. Assign the result to 'diagnosis_knn'.
diagnosis_knn <- knn(train = train[-1], test = test[-1], cl = train$diagnosis, k = 22)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn'.
confusion_knn <- table(test$diagnosis, diagnosis_knn)
confusion_knn
# Rescale
# Conduct a KNN classification of the diagnosis variable, set the k parameter to 21. Assign the result to 'diagnosis_knn'.
diagnosis_knn <- knn(train = train[-1], test = test[-1], cl = train$diagnosis, k = 21)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn'.
confusion_knn <- table(test$diagnosis, diagnosis_knn)
confusion_knn
glimpse(train)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# Load all the libraries we need.
library(tidyverse)
library(stringr)
library(rpart)
library(partykit)
library(randomForest)
library(class)
# Load the data set. Assign it to the variable 'fna'.
fna <- read_csv("FNA_cancer.csv")
glimpse(fna)
# Make a copy of the original data set and delete the first and last column which are useless. Assign it with a new variable 'fna_new'.
fna_new <- data.frame(fna)
fna_new <- fna_new %>% select(-id, -X33)
glimpse(fna_new)
# Replace the dot sign in column names with underscore so that all column names are in uniform form.
names(fna_new) <- str_replace_all(names(fna_new), '[.]', '_')
# Conver the response variable 'diagnosis' into factor.
fna_new$diagnosis <- as.factor(fna_new$diagnosis)
glimpse(fna_new)
# Create a vecor of 10 features of the cell nuclei.
features <- c('radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension')
# Iterate over the 10 features of cell nuclei, and print the numeric summary of their accordingly mean, standard error of the mean, and the maximum.
for (i in 1:10) {
for (j in seq_along(names(fna_new))) {
if (str_detect(names(fna_new)[j], features[i])) {
print(summary(fna_new[names(fna_new)[j]]))
}
}
}
summary(fna_new)
# Set the seed to 1847
set.seed(1847)
# Get the total rows of the dataset. Assign the result to the variable 'n'.
n <- nrow(fna_new)
# Randomly generate the indexes for the test set. Use the indexes to select the 20% test set. Assign the test set to the variable 'test'.
test_idx <- sample.int(n, size = round(0.2 * n))
test <- fna_new[test_idx, ]
# Select the rest 80% training data and assign it to the variable 'train'.
train <- fna_new[-test_idx, ]
# Give a glimpse of the resulting training set.
glimpse(train)
# Give a glimpse of the resulting testing set.
glimpse(test)
# Create the regression formula, take 'diagnosis' as the response variable and all the rest 30 characteristics as the predictor variables. Assign the formula to the variable 'form'.
form <- as.formula(diagnosis ~ .)
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree'.
diagnosis_tree <- rpart(form, data = train)
plot(as.party(diagnosis_tree))
# Use 'diagnosis_tree' model to predict the test data and assign the predictions to the variable 'test_pred_tree'.
test_pred_tree <- predict(diagnosis_tree, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree'.
confusion_tree <- table(test$diagnosis, test_pred_tree)
confusion_tree
printcp(diagnosis_tree)
form_mean <- as.formula("diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + concave_points_mean + symmetry_mean + fractal_dimension_mean")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_mean'.
diagnosis_tree_mean <- rpart(form_mean, data = train)
plot(as.party(diagnosis_tree_mean))
# Use 'diagnosis_tree' model to predict the test data and assign the predictions to the variable 'test_pred_tree_mean'.
test_pred_tree_mean <- predict(diagnosis_tree_mean, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_mean'.
confusion_tree_mean <- table(test$diagnosis, test_pred_tree_mean)
confusion_tree_mean
printcp(diagnosis_tree_mean)
# Create a bagged(so mtry = 30) set of trees from the training data set. Assign the result to the variable 'diagnosis_bagging'.
diagnosis_bagging <- randomForest(form, data = train, mtry = 30, ntree = 500, na.action = na.roughfix)
diagnosis_bagging
# Use 'diagnosis_bagging' model to predict the test data and assign the predictions to the variable 'test_pred_bagging'.
test_pred_bagging <- predict(diagnosis_bagging, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_bagging'.
confusion_bagging <- table(test$diagnosis, test_pred_bagging)
confusion_bagging
# Create a random forest from the training data set, set the mtry = 5. Assign the result to the variable 'diagnosis_forest'.
diagnosis_forest <- randomForest(form, data = train, mtry = 5, ntree = 500, na.action = na.roughfix)
diagnosis_forest
# Use 'diagnosis_forest' model to predict the test data and assign the predictions to the variable 'test_pred_forest'.
test_pred_forest <- predict(diagnosis_forest, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_forest
confusion_forest <- table(test$diagnosis, test_pred_forest)
confusion_forest
# Rescale
# Conduct a KNN classification of the diagnosis variable, set the k parameter to 21. Assign the result to 'diagnosis_knn'.
diagnosis_knn <- knn(train = train[-1], test = test[-1], cl = train$diagnosis, k = 21)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn'.
confusion_knn <- table(test$diagnosis, diagnosis_knn)
confusion_knn
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# Load the library
library(tidyverse)
library(NHANES)
library(class)
# Load the dataset.
dat <- data.frame(NHANES)
# Create a 1-0 indicator variable for the RegularMarij and gender variables. Name these variables RM1 and Sex1.
dat$RM1 <- ifelse(dat$RegularMarij == 'Yes', 1, 0)
dat$Sex1 <- ifelse(dat$Gender == 'male', 1, 0)
# Summary of the 'RM1'
summary(dat$RM1)
# Summary of the 'Sex1'
summary(dat$Sex1)
# Limit the data set only to several variables. Assign the limited data set to the variable 'dat2'.
dat2 <- dat %>% select(HardDrugs, RM1, Age, AlcoholYear, BMI, Sex1, HHIncomeMid, Weight, Height, SexNumPartnLife, TotChol)
glimpse(dat2)
# Remove missing values from 'dat2', assign the new data set to the variable 'dat3'.
dat3 <- na.omit(dat2)
# Provide a count of the rows of the data remaining in the dataset.
nrow(dat3)
# Set the seed to 1847
set.seed(1847)
# Get the total rows of the dataset. Assign the result to the variable 'n'.
n <- nrow(dat3)
# Randomly generate the indexes for the test set. Use the indexes to select the 20% test set. Assign the test set to the variable 'test'.
test_idx <- sample.int(n, size = round(0.2 * n))
test <- dat3[test_idx, ]
# Select the rest 80% training data and assign it to the variable 'train'.
train <- dat3[-test_idx, ]
# Give a glimpse of the resulting training set.
glimpse(train)
# Give a glimpse of the resulting testing set.
glimpse(test)
# Create a rescale function, name it 'rescale_x'.
rescale_x <- function(x){(x - min(x)) / (max(x) - min(x))}
# Rescale the training data. Assign the rescaled test data to the cariable 'new_train'.
new_train <- train
new_train[-1] <- rescale_x(train[-1])
glimpse(new_train)
# Create a rescale function, name it 'rescale_xy'.
rescale_xy <- function(x, y){(x - min(y)) / (max(y) - min(y))}
# Rescale the test data. Assign the rescaled test data to the cariable 'new_test'.
new_test <- test
new_test[-1] <- rescale_xy(test[-1], train[-1])
glimpse(new_test)
# Conduct a KNN classification of the HardDrugs variable, set the k parameter to 1. Assign the result to 'harddrugs_knn1'.
harddrugs_knn1 <- knn(train = new_train[-1], test = new_test[-1], cl = new_train$HardDrugs, k = 1)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn1'.
confusion_knn1 <- table(harddrugs_knn1, test$HardDrugs)
confusion_knn1
# Conduct a KNN classification of the HardDrugs variable, set the k parameter to 5. Assign the result to 'harddrugs_knn5'.
harddrugs_knn5 <- knn(train = new_train[-1], test = new_test[-1], cl = new_train$HardDrugs, k = 5)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn5'.
confusion_knn5 <- table(harddrugs_knn5, test$HardDrugs)
confusion_knn5
# Conduct a KNN classification of the HardDrugs variable, set the k parameter to 25. Assign the result to 'harddrugs_knn25'.
harddrugs_knn25 <- knn(train = new_train[-1], test = new_test[-1], cl = new_train$HardDrugs, k = 25)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn25'.
confusion_knn25 <- table(harddrugs_knn25, test$HardDrugs)
confusion_knn25
# Conduct a KNN classification of the HardDrugs variable, set the k parameter to 100. Assign the result to 'harddrugs_knn100.'.
harddrugs_knn100 <- knn(train = new_train[-1], test = new_test[-1], cl = new_train$HardDrugs, k = 100)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn100'.
confusion_knn100 <- table(harddrugs_knn100, test$HardDrugs)
confusion_knn100
# Set the seed to 1847.
set.seed(1847)
# Use 'runif' function to randomly generate 500 observations on two simulated predictor variables called x1 and x2 that uniformly distributed between 0 and 10.
x1 <- runif(n = 500, min = 0, max = 10)
x2 <- runif(n = 500, min = 0, max = 10)
# Use sample function to randomly generate 500 observations that has values 1 for “yes” and 0 for “no”. Assign the simulated response variable to 'y'.
y <- sample(c(1, 0), size = 500, replace = TRUE)
# Use 'cbind.data.frame' function to combine the variables x1, x2 and y into a data frame. Assign the result to the variable 'df1'.
df1 <- cbind.data.frame(x1, x2, y)
glimpse(df1)
# Set the seed to 1847.
set.seed(1847)
# Randomly generate the indexes for the test set. Use the indexes to select the 20% test set. Assign the test set to the variable 'test2'.
test_idx2 <- sample.int(nrow(df1), size = round(0.2 * nrow(df1)))
test2 <- df1[test_idx2, ]
# Select the rest 80% training data and assign it to the variable 'train2'.
train2 <- df1[-test_idx2, ]
glimpse(train2)
# Use expand.grid to create a grid of points corresponding to x1 and x2 from 0 to 10 with intervals 0.1. Assign the result to the variable 'grid1'.
grid1 <- expand.grid(x1 = seq(0, 10, 0.1), x2 = seq(0, 10, 0.1))
glimpse(grid1)
# Conduct a KNN classification with K = 25. Assign the predictions to the variable 'pred_knn25'.
pred_knn25 <- knn(train = train2[-3], test = grid1, cl = train2$y, k = 25)
head(pred_knn25)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid1, aes(x = x1, y = x2, color = pred_knn25)) + geom_point()
# Conduct a KNN classification with K = 5. Assign the predictions to the variable 'pred_knn5'.
pred_knn5 <- knn(train = train2[-3], test = grid1, cl = train2$y, k = 5)
head(pred_knn5)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid1, aes(x = x1, y = x2, color = pred_knn5)) + geom_point()
# Conduct a KNN classification with K = 1. Assign the predictions to the variable 'pred_knn1'.
pred_knn1 <- knn(train = train2[-3], test = grid1, cl = train2$y, k = 1)
head(pred_knn1)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid1, aes(x = x1, y = x2, color = pred_knn1)) + geom_point()
# Conduct a KNN classification with K = 400. Assign the predictions to the variable 'pred_knn400'.
pred_knn400 <- knn(train = train2[-3], test = grid1, cl = train2$y, k = 400)
head(pred_knn400)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid1, aes(x = x1, y = x2, color = pred_knn400)) + geom_point()
# Make a data set with two groups
# Make group 1
x1 <- rnorm(250, 6, 1)
x2 <- rnorm(250, 6, 1)
y <- rep(1, 250)
g1 <- cbind.data.frame(x1, x2, y)
glimpse(g1)
# Make group 2
x1 <- rnorm(250, 5, 1)
x2 <- rnorm(250, 5, 1)
y <- rep(0, 250)
g2 <- cbind.data.frame(x1, x2, y)
glimpse(g2)
# Combine the dataframes
sim_dat3 <- rbind(g1, g2)
glimpse(sim_dat3)
# Create a scatterplot, map x1 to the x aesthetic, map x2 to the y aesthetic, map y to the color aesthetic.
ggplot(sim_dat3, aes(x = x1, y = x2, color = as.factor(y))) + geom_point()
# Set the seed to 1847.
set.seed(1847)
# Randomly generate the indexes for the test set. Use the indexes to select the 20% test set. Assign the test set to the variable 'test3'.
test_idx3 <- sample.int(nrow(sim_dat3), size = round(0.2 * nrow(sim_dat3)))
test3 <- sim_dat3[test_idx3, ]
# Select the rest 80% training data and assign it to the variable 'train3'.
train3 <- sim_dat3[-test_idx3, ]
glimpse(train3)
# Use expand.grid to create a grid of points corresponding to x1 and x2 from 0 to 10 with intervals 0.1. Assign the result to the variable 'grid2'.
grid2 <- expand.grid(x1 = seq(0, 10, 0.1), x2 = seq(0, 10, 0.1))
glimpse(grid2)
# Conduct a KNN classification with K = 25. Assign the predictions to the variable 'pred2_knn25'.
pred2_knn25 <- knn(train = train3[-3], test = grid2, cl = train3$y, k = 25)
head(pred2_knn25)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid2, aes(x = x1, y = x2, color = pred2_knn25)) + geom_point()
# Conduct a KNN classification with K = 5. Assign the predictions to the variable 'pred2_knn5'.
pred2_knn5 <- knn(train = train3[-3], test = grid2, cl = train3$y, k = 5)
head(pred2_knn5)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid2, aes(x = x1, y = x2, color = pred2_knn5)) + geom_point()
# Conduct a KNN classification with K = 1. Assign the predictions to the variable 'pred2_knn1'.
pred2_knn1 <- knn(train = train3[-3], test = grid2, cl = train3$y, k = 1)
head(pred2_knn1)
#  Create a scatterplot of the grid and represent the predicted values (1 or 0) with color.
ggplot(grid2, aes(x = x1, y = x2, color = pred2_knn1)) + geom_point()
# Conduct a KNN classification with K = 25 to the test data set, assign the prediction to the variable 'pred_test3_knn25'.
pred_test3_knn25 <- knn(train = train3[-3], test = test3[-3], cl = train3$y, k = 25)
# Get the confusion matrix.
table(pred_test3_knn25, test3$y)
# Conduct a KNN classification with K = 5 to the test data set, assign the prediction to the variable 'pred_test3_knn5'.
pred_test3_knn5 <- knn(train = train3[-3], test = test3[-3], cl = train3$y, k = 5)
# Get the confusion matrix.
table(pred_test3_knn5, test3$y)
# Conduct a KNN classification with K = 1 to the test data set, assign the prediction to the variable 'pred_test3_knn1'.
pred_test3_knn1 <- knn(train = train3[-3], test = test3[-3], cl = train3$y, k = 1)
# Get the confusion matrix.
table(pred_test3_knn1, test3$y)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# Load all the libraries we need.
library(tidyverse)
library(stringr)
library(rpart)
library(partykit)
library(randomForest)
library(class)
# Load the data set. Assign it to the variable 'fna'.
fna <- read_csv("FNA_cancer.csv")
# Load the data set. Assign it to the variable 'fna'.
fna <- read_csv("FNA_cancer.csv")
glimpse(fna)
# Make a copy of the original data set and delete the first and last column which are useless. Assign it with a new variable 'fna_new'.
fna_new <- data.frame(fna)
fna_new <- fna_new %>% select(-id, -X33)
glimpse(fna_new)
# Replace the dot sign in column names with underscore so that all column names are in uniform form.
names(fna_new) <- str_replace_all(names(fna_new), '[.]', '_')
# Conver the response variable 'diagnosis' into factor.
fna_new$diagnosis <- as.factor(fna_new$diagnosis)
glimpse(fna_new)
# Create a vecor of 10 features of the cell nuclei.
features <- c('radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension')
# Iterate over the 10 features of cell nuclei, and print the numeric summary of their accordingly mean, standard error of the mean, and the maximum.
for (i in 1:10) {
for (j in seq_along(names(fna_new))) {
if (str_detect(names(fna_new)[j], features[i])) {
print(summary(fna_new[names(fna_new)[j]]))
}
}
}
# Set the seed to 1847
set.seed(1847)
# Get the total rows of the dataset. Assign the result to the variable 'n'.
n <- nrow(fna_new)
# Randomly generate the indexes for the test set. Use the indexes to select the 20% test set. Assign the test set to the variable 'test'.
test_idx <- sample.int(n, size = round(0.2 * n))
test <- fna_new[test_idx, ]
# Select the rest 80% training data and assign it to the variable 'train'.
train <- fna_new[-test_idx, ]
# Give a glimpse of the resulting training set.
glimpse(train)
# Give a glimpse of the resulting testing set.
glimpse(test)
# Create the regression formula, take 'diagnosis' as the response variable and all the rest 30 characteristics as the predictor variables. Assign the formula to the variable 'form'.
form <- as.formula(diagnosis ~ .)
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree'.
diagnosis_tree <- rpart(form, data = train)
plot(as.party(diagnosis_tree))
# Use 'diagnosis_tree' model to predict the test data and assign the predictions to the variable 'test_pred_tree'.
test_pred_tree <- predict(diagnosis_tree, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree'.
confusion_tree <- table(test$diagnosis, test_pred_tree)
confusion_tree
printcp(diagnosis_tree)
# Create the regression formula, take 'diagnosis' as the response variable and the 10 mean value of the characteristics as the predictor variables. Assign the formula to the variable 'form_mean'.
form_mean <- as.formula("diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + concave_points_mean + symmetry_mean + fractal_dimension_mean")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_mean'.
diagnosis_tree_mean <- rpart(form_mean, data = train)
plot(as.party(diagnosis_tree_mean))
# Use 'diagnosis_tree' model to predict the test data and assign the predictions to the variable 'test_pred_tree_mean'.
test_pred_tree_mean <- predict(diagnosis_tree_mean, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_mean'.
confusion_tree_mean <- table(test$diagnosis, test_pred_tree_mean)
confusion_tree_mean
printcp(diagnosis_tree_mean)
glimpse(train)
# Create the regression formula, take 'diagnosis' as the response variable and the 10 standard error of the mean of the characteristics as the predictor variables. Assign the formula to the variable 'form_se'.
form_se <- as.formula("diagnosis ~ radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave_points_se + symmetry_se + fractal_dimension_se")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_se'.
diagnosis_tree_se <- rpart(form_se, data = train)
plot(as.party(diagnosis_tree_se))
# Use 'diagnosis_tree_se' model to predict the test data and assign the predictions to the variable 'test_pred_tree_sd'.
test_pred_tree_sd <- predict(diagnosis_tree_se, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_sd'.
confusion_tree_sd <- table(test$diagnosis, test_pred_tree_sd)
confusion_tree_sd
printcp(diagnosis_tree_se)
glimpse(train)
# Create the regression formula, take 'diagnosis' as the response variable and the 10 maximum value of the characteristics as the predictor variables. Assign the formula to the variable 'form_worst'.
form_worst <- as.formula("diagnosis ~ radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst + concave_points_worst + symmetry_worst + fractal_dimension_worst")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_worst'.
diagnosis_tree_worst <- rpart(form_worst, data = train)
plot(as.party(diagnosis_tree_worst))
# Use 'diagnosis_tree_worst' model to predict the test data and assign the predictions to the variable 'test_pred_tree_worst'.
test_pred_tree_worst <- predict(diagnosis_tree_worst, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_worst'.
confusion_tree_worst <- table(test$diagnosis, test_pred_tree_worst)
confusion_tree_worst
printcp(diagnosis_tree_worst)
glimpse(train)
# Create a rescale function, name it 'rescale_x'.
rescale_x <- function(x){(x - min(x)) / (max(x) - min(x))}
# Rescale the training data. Assign the rescaled test data to the cariable 'new_train'.
new_train <- train
new_train[-1] <- rescale_x(train[-1])
glimpse(new_train)
# Create a rescale function, name it 'rescale_xy'.
rescale_xy <- function(x, y){(x - min(y)) / (max(y) - min(y))}
# Rescale the test data. Assign the rescaled test data to the cariable 'new_test'.
new_test <- test
new_test[-1] <- rescale_xy(test[-1], train[-1])
glimpse(new_test)
# Conduct a KNN classification of the diagnosis variable, set the k parameter to 21. Assign the result to 'diagnosis_knn'.
diagnosis_knn <- knn(train = new_train[-1], test = new_test[-1], cl = new_train$diagnosis, k = 21)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn'.
confusion_knn <- table(test$diagnosis, diagnosis_knn)
confusion_knn
# Create a random forest from the training data set, set the mtry = 5. Assign the result to the variable 'diagnosis_forest'.
diagnosis_forest <- randomForest(form, data = train, mtry = 5, ntree = 500, na.action = na.roughfix)
diagnosis_forest
# Use 'diagnosis_forest' model to predict the test data and assign the predictions to the variable 'test_pred_forest'.
test_pred_forest <- predict(diagnosis_forest, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_forest
confusion_forest <- table(test$diagnosis, test_pred_forest)
confusion_forest
# Create a bagged(so mtry = 30) set of trees from the training data set. Assign the result to the variable 'diagnosis_bagging'.
diagnosis_bagging <- randomForest(form, data = train, mtry = 30, ntree = 500, na.action = na.roughfix)
diagnosis_bagging
# Use 'diagnosis_bagging' model to predict the test data and assign the predictions to the variable 'test_pred_bagging'.
test_pred_bagging <- predict(diagnosis_bagging, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_bagging'.
confusion_bagging <- table(test$diagnosis, test_pred_bagging)
confusion_bagging
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# Load all the libraries we need.
library(tidyverse)
library(stringr)
library(rpart)
library(partykit)
library(randomForest)
library(class)
# Load the data set. Assign it to the variable 'fna'.
fna <- read_csv("FNA_cancer.csv")
glimpse(fna)
# Make a copy of the original data set and delete the first and last column which are useless. Assign it with a new variable 'fna_new'.
fna_new <- data.frame(fna)
fna_new <- fna_new %>% select(-id, -X33)
glimpse(fna_new)
# Replace the dot sign in column names with underscore so that all column names are in uniform form.
names(fna_new) <- str_replace_all(names(fna_new), '[.]', '_')
# Conver the response variable 'diagnosis' into factor.
fna_new$diagnosis <- as.factor(fna_new$diagnosis)
glimpse(fna_new)
# Create a vecor of 10 features of the cell nuclei.
features <- c('radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension')
# Iterate over the 10 features of cell nuclei, and print the numeric summary of their accordingly mean, standard error of the mean, and the maximum.
for (i in 1:10) {
for (j in seq_along(names(fna_new))) {
if (str_detect(names(fna_new)[j], features[i])) {
print(summary(fna_new[names(fna_new)[j]]))
}
}
}
summary(fna_new)
# Set the seed to 1847
set.seed(1847)
# Get the total rows of the dataset. Assign the result to the variable 'n'.
n <- nrow(fna_new)
# Randomly generate the indexes for the test set. Use the indexes to select the 20% test set. Assign the test set to the variable 'test'.
test_idx <- sample.int(n, size = round(0.2 * n))
test <- fna_new[test_idx, ]
# Select the rest 80% training data and assign it to the variable 'train'.
train <- fna_new[-test_idx, ]
# Give a glimpse of the resulting training set.
glimpse(train)
# Give a glimpse of the resulting testing set.
glimpse(test)
# Create the regression formula, take 'diagnosis' as the response variable and all the rest 30 characteristics as the predictor variables. Assign the formula to the variable 'form'.
form <- as.formula(diagnosis ~ .)
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree'.
diagnosis_tree <- rpart(form, data = train)
plot(as.party(diagnosis_tree))
# Use 'diagnosis_tree' model to predict the test data and assign the predictions to the variable 'test_pred_tree'.
test_pred_tree <- predict(diagnosis_tree, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree'.
confusion_tree <- table(test$diagnosis, test_pred_tree)
confusion_tree
printcp(diagnosis_tree)
# Create the regression formula, take 'diagnosis' as the response variable and the 10 mean value of the characteristics as the predictor variables. Assign the formula to the variable 'form_mean'.
form_mean <- as.formula("diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + concave_points_mean + symmetry_mean + fractal_dimension_mean")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_mean'.
diagnosis_tree_mean <- rpart(form_mean, data = train)
plot(as.party(diagnosis_tree_mean))
# Use 'diagnosis_tree_mean' model to predict the test data and assign the predictions to the variable 'test_pred_tree_mean'.
test_pred_tree_mean <- predict(diagnosis_tree_mean, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_mean'.
confusion_tree_mean <- table(test$diagnosis, test_pred_tree_mean)
confusion_tree_mean
printcp(diagnosis_tree_mean)
# Create the regression formula, take 'diagnosis' as the response variable and the 10 standard error of the mean of the characteristics as the predictor variables. Assign the formula to the variable 'form_se'.
form_se <- as.formula("diagnosis ~ radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + concave_points_se + symmetry_se + fractal_dimension_se")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_se'.
diagnosis_tree_se <- rpart(form_se, data = train)
plot(as.party(diagnosis_tree_se))
# Use 'diagnosis_tree_se' model to predict the test data and assign the predictions to the variable 'test_pred_tree_sd'.
test_pred_tree_sd <- predict(diagnosis_tree_se, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_sd'.
confusion_tree_sd <- table(test$diagnosis, test_pred_tree_sd)
confusion_tree_sd
printcp(diagnosis_tree_se)
# Create the regression formula, take 'diagnosis' as the response variable and the 10 maximum value of the characteristics as the predictor variables. Assign the formula to the variable 'form_worst'.
form_worst <- as.formula("diagnosis ~ radius_worst + texture_worst + perimeter_worst + area_worst + smoothness_worst + compactness_worst + concavity_worst + concave_points_worst + symmetry_worst + fractal_dimension_worst")
# Create a tree using the formula just created. Assign the new tree to the variable 'diagnosis_tree_worst'.
diagnosis_tree_worst <- rpart(form_worst, data = train)
plot(as.party(diagnosis_tree_worst))
# Use 'diagnosis_tree_worst' model to predict the test data and assign the predictions to the variable 'test_pred_tree_worst'.
test_pred_tree_worst <- predict(diagnosis_tree_worst, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_tree_worst'.
confusion_tree_worst <- table(test$diagnosis, test_pred_tree_worst)
confusion_tree_worst
printcp(diagnosis_tree_worst)
# Create a bagged(so mtry = 30) set of trees from the training data set. Assign the result to the variable 'diagnosis_bagging'.
diagnosis_bagging <- randomForest(form, data = train, mtry = 30, ntree = 500, na.action = na.roughfix)
diagnosis_bagging
# Use 'diagnosis_bagging' model to predict the test data and assign the predictions to the variable 'test_pred_bagging'.
test_pred_bagging <- predict(diagnosis_bagging, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_bagging'.
confusion_bagging <- table(test$diagnosis, test_pred_bagging)
confusion_bagging
# Create a random forest from the training data set, set the mtry = 5. Assign the result to the variable 'diagnosis_forest'.
diagnosis_forest <- randomForest(form, data = train, mtry = 5, ntree = 500, na.action = na.roughfix)
diagnosis_forest
# Use 'diagnosis_forest' model to predict the test data and assign the predictions to the variable 'test_pred_forest'.
test_pred_forest <- predict(diagnosis_forest, newdata = test, type = 'class')
# Create the confusion matrix for the test data set. Assign the result to the variable 'confusion_forest
confusion_forest <- table(test$diagnosis, test_pred_forest)
confusion_forest
# Create a rescale function, name it 'rescale_x'.
rescale_x <- function(x){(x - min(x)) / (max(x) - min(x))}
# Rescale the training data. Assign the rescaled test data to the cariable 'new_train'.
new_train <- train
new_train[-1] <- rescale_x(train[-1])
glimpse(new_train)
# Create a rescale function, name it 'rescale_xy'.
rescale_xy <- function(x, y){(x - min(y)) / (max(y) - min(y))}
# Rescale the test data. Assign the rescaled test data to the cariable 'new_test'.
new_test <- test
new_test[-1] <- rescale_xy(test[-1], train[-1])
glimpse(new_test)
# Conduct a KNN classification of the diagnosis variable, set the k parameter to 21. Assign the result to 'diagnosis_knn'.
diagnosis_knn <- knn(train = new_train[-1], test = new_test[-1], cl = new_train$diagnosis, k = 21)
# Calculate the confusion matrix for the test data. Assign the result to the variable 'confusion_knn'.
confusion_knn <- table(test$diagnosis, diagnosis_knn)
confusion_knn
